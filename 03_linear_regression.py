#!usr/bin/env python
# -*- coding:utf-8 _*-
"""
@version:
author:yanqiang
@time: 2019/04/09
@file: main.py
@description:çº¿æ€§å›å½’
"""
import torch
from time import time
print(torch.__version__)

a=torch.ones(1000)
b=torch.ones(1000)

# å°†è¿™ä¸¤ä¸ªå‘é‡æŒ‰å…ƒç´ é€ä¸€åšæ ‡é‡åŠ æ³•:
start=time()
c=torch.zeros(1000)
for i in range(1000):
    c[i]=a[i]+b[i]
print(time()-start)

# ä¸¤ä¸ªå‘é‡ç›´æ¥ç›¸åŠ 
start=time()
d=a+b
print(time()-start)

# å¹¿æ’­æœºåˆ¶ä¾‹å­ğŸŒ°ï¼š
a=torch.ones(3)
b=10
print(a+b)

###################################33
# çº¿æ€§å›å½’çš„ç®€ä»‹å®ç°
import torch
from torch import nn
import numpy as np

print(torch.__version__)

# ç”Ÿæˆæ•°æ®é›†
num_inputs=2
num_examples=1000
true_w=[2,-3.4]
true_b=4.2

features=torch.tensor(np.random.normal(0,1,(num_examples,num_inputs)),dtype=torch.float)
labels=true_w[0]*features[:,0]+true_w[1]*features[:,1]+b
# æ·»åŠ å™ªéŸ³
labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)

# è¯»å–æ•°æ®
import torch.utils.data as Data
batch_size=10

# å°†è®­ç»ƒæ•°æ®çš„ç‰¹å¾ä¸æ ‡ç­¾ç»„åˆ
datastet=Data.TensorDataset(features,labels)

# å°†datasetæ”¾å…¥DataLoader
data_iter=Data.DataLoader(
    dataset=datastet,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2,
)
for X,y in data_iter:
    print(X,'\n',y)
    break

# å®šä¹‰æ¨¡å‹
class LinearNet(nn.Module):
    def __init__(self,n_features):
        """
        åˆå§‹åŒ–
        :param n_features: ç‰¹å¾ä¸ªæ•°ï¼Œè¾“å…¥ç»´åº¦
        """
        super(LinearNet, self).__init__()
        self.linear=nn.Linear(n_features,1)
    def forward(self,x):
        y=self.linear(x)
        return y
net=LinearNet(num_inputs)
print(net)

# =============== å…¶ä»–å†™æ³• ===================

# å†™æ³•ä¸€
net = nn.Sequential(
    nn.Linear(num_inputs, 1)
    # æ­¤å¤„è¿˜å¯ä»¥ä¼ å…¥å…¶ä»–å±‚
    )

# å†™æ³•äºŒ
net = nn.Sequential()
net.add_module('linear', nn.Linear(num_inputs, 1))
# net.add_module ......

# å†™æ³•ä¸‰
from collections import OrderedDict
net = nn.Sequential(OrderedDict([
          ('linear', nn.Linear(num_inputs, 1))
          # ......
        ]))

print(net)
print(net[0])


for param in net.parameters():
    print(param)

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
from torch.nn import init
init.normal_(net[0].weight,mean=0.0,std=0.1)
init.constant_(net[0].bias,val=0.0)

for param in net.parameters():
    print(param)

# å®šä¹‰æŸå¤±å‡½æ•°
loss=nn.MSELoss()
# å®šä¹‰ä¼˜åŒ–ç®—æ³•
import  torch.optim as optim
optimizer=optim.SGD(net.parameters(),lr=0.03)
print(optimizer)


# ä¸ºä¸åŒå­ç½‘ç»œè®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
# optimizer =optim.SGD([
#                 # å¦‚æœå¯¹æŸä¸ªå‚æ•°ä¸æŒ‡å®šå­¦ä¹ ç‡ï¼Œå°±ä½¿ç”¨æœ€å¤–å±‚çš„é»˜è®¤å­¦ä¹ ç‡
#                 {'params': net.subnet1.parameters()}, # lr=0.03
#                 {'params': net.subnet2.parameters(), 'lr': 0.01}
#             ], lr=0.03)

# è°ƒæ•´å­¦ä¹ ç‡
# for param_group in optimizer.param_groups:
#     param_group['lr'] *= 0.1 # å­¦ä¹ ç‡ä¸ºä¹‹å‰çš„0.1å€

# è®­ç»ƒæ¨¡å‹
num_epochs=3
for epoch in range(1,num_epochs+1):
    for X,y in data_iter:
        output=net(X)
        l=loss(output,y.view(-1,1))
        optimizer.zero_grad() # æ¢¯åº¦æ¸…é›¶ï¼Œç­‰ä»·äºnet.zeo_grad()
        l.backward()
        optimizer.step()
    format='epoch %d, loss: %f'
    print(format % (epoch,l.item()))

dense=net[0]
print(true_w,dense.weight.data)
print(true_b,dense.bias.data)

# [2, -3.4] tensor([[ 1.9994, -3.3995]])
# 4.2 tensor([10.0001])